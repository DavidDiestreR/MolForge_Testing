{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db389858",
   "metadata": {},
   "source": [
    "# Evaluación sencilla de MolForge con RDKit\n",
    "_Notebook minimalista para probar rápidamente la **precisión (top‑1, Tc=1.0)** de un modelo MolForge ya entrenado._\n",
    "\n",
    "Este cuaderno:\n",
    "1) Recibe una **lista de SMILES** y un **tipo de fingerprint** de entrada.\n",
    "2) Convierte cada SMILES a fingerprint con **RDKit** y lo tokeniza.\n",
    "3) Llama a **MolForge** para decodificar fingerprint → SMILES (o SELFIES).\n",
    "4) Calcula la **precisión** como el % de casos con **Tc=1.0** (Tanimoto exact) usando **Morgan sparse r=1** para evaluar.\n",
    "\n",
    "**Código comentado en inglés.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee48d0d",
   "metadata": {},
   "source": [
    "## 0) Device — detección automática CPU/GPU\n",
    "<small>El entorno ya está preparado con `environment.yml`. Aquí detectamos el **device** automáticamente. Puedes forzarlo con `preferred=\"cpu\"` o mediante variable de entorno `DEVICE`.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e123b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device detection (import from src if available; otherwise define here)\n",
    "try:\n",
    "    from src.utils_device import pick_device  # prefer project utility if present\n",
    "except Exception:\n",
    "    def pick_device(preferred: str | None = None) -> str:\n",
    "        \"\"\"Return 'cuda', 'mps' (Apple), or 'cpu'. Supports override with preferred.\"\"\"\n",
    "        if preferred in {\"cpu\", \"cuda\", \"mps\"}:\n",
    "            return preferred\n",
    "        try:\n",
    "            import torch\n",
    "        except Exception:\n",
    "            return \"cpu\"\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            import torch\n",
    "            if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "                return \"mps\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"cpu\"\n",
    "\n",
    "import os\n",
    "device = pick_device(os.getenv(\"DEVICE\"))  # set DEVICE=cpu|cuda|mps to override\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862440f",
   "metadata": {},
   "source": [
    "## 1) Input (edita aquí)\n",
    "<small>Define los SMILES de prueba, el fingerprint de **entrada** (p. ej., `ECFP4`) y la configuración del modelo MolForge (nombre/ckpt).</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1045a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Dummy SMILES to quickly run once you connect MolForge ---\n",
    "SMILES_LIST = [\n",
    "    \"CCO\",                       # ethanol\n",
    "    \"c1ccccc1\",                  # benzene\n",
    "    \"CC(=O)OC1=CC=CC=C1C(=O)O\",  # aspirin\n",
    "    \"CN1C=NC2=C1C(=O)N(C(=O)N2)C\",  # caffeine\n",
    "    \"CC(=O)O\",                   # acetic acid\n",
    "]\n",
    "\n",
    "# Fingerprint to FEED MolForge (input side). Common options: 'ECFP4', 'AEs', 'TT', 'HashAP', 'RDK4', 'MACCS', 'FCFP4', ...\n",
    "INPUT_FP = \"ECFP4\"\n",
    "\n",
    "# What MolForge will generate: 'SMILES' or 'SELFIES'\n",
    "OUTPUT_REPR = \"SMILES\"\n",
    "\n",
    "# MolForge model configuration (fill these with your real paths/names)\n",
    "MODEL_NAME = \"ecfp4_to_smiles\"                 # adapt to your MolForge model naming\n",
    "MOLFORGE_CHECKPOINT = \"/path/to/model.ckpt\"    # put your real checkpoint path\n",
    "\n",
    "# Bit length for hashed fingerprints (used by ECFP/FCFP/HashAP/HashTT/RDK4/etc.)\n",
    "HASHED_NBITS = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c65f94",
   "metadata": {},
   "source": [
    "## 2) Asunciones de entorno y conexión\n",
    "<small>Este notebook asume que has creado y activado el entorno con `environment.yml` y que **MolForge** ya está instalado vía `pip install git+https://github.com/knu-lcbc/MolForge.git`. No hace falta repetir instalaciones aquí. Solo asegúrate de tener el **checkpoint** correcto, y el modelo podrá correr en `cpu` o `cuda` según lo detectado arriba.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb1e3e",
   "metadata": {},
   "source": [
    "## 3) Imports\n",
    "<small>Importamos RDKit y utilidades. Si SELFIES es la salida, instala `selfies` para convertir a SMILES al evaluar.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b811857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MolForge as molforge\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Optional, Tuple\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, MACCSkeys, rdMolDescriptors as rdDesc\n",
    "from rdkit import DataStructs\n",
    "\n",
    "try:\n",
    "    import selfies as sf\n",
    "    SELFIES_OK = True\n",
    "except Exception:\n",
    "    SELFIES_OK = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856af38",
   "metadata": {},
   "source": [
    "## 4) Utilidades básicas (parseo y canonización)\n",
    "<small>Funciones pequeñas para pasar de SMILES a moléculas RDKit y obtener SMILES canónicos.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1407141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_mol(smiles: str):\n",
    "    \"\"\"Parse SMILES into an RDKit Mol with sanitization; return None on failure.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        Chem.SanitizeMol(mol)\n",
    "        return mol\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def canon_smiles(mol, isomeric: bool = True) -> str:\n",
    "    \"\"\"Return canonical SMILES (isomeric if requested).\"\"\"\n",
    "    if mol is None:\n",
    "        return \"\"\n",
    "    return Chem.MolToSmiles(mol, isomericSmiles=isomeric, canonical=True)\n",
    "\n",
    "def selfies_to_smiles(s: str) -> str:\n",
    "    \"\"\"Convert SELFIES -> SMILES if selfies is installed; otherwise return input.\"\"\"\n",
    "    if not SELFIES_OK:\n",
    "        return s\n",
    "    try:\n",
    "        return sf.decoder(s)\n",
    "    except Exception:\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6dc5bb",
   "metadata": {},
   "source": [
    "## 5) Fingerprints de RDKit (entrada y evaluación)\n",
    "<small>Implementamos los fingerprints de **entrada** y también el fingerprint de **evaluación** Morgan sparse r=1 usado para Tc.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9865883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class FPResult:\n",
    "    kind: str\n",
    "    obj: Any\n",
    "    is_sparse: bool\n",
    "    nbits: Optional[int] = None\n",
    "\n",
    "# ---- Input FPs (to FEED MolForge) ----\n",
    "def fp_morgan_hashed(mol, radius: int, nBits: int, useFeatures: bool = False) -> FPResult:\n",
    "    bv = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, useFeatures=useFeatures)\n",
    "    kind = f\"{'FCFP' if useFeatures else 'ECFP'}{2*radius}\"\n",
    "    return FPResult(kind=kind, obj=bv, is_sparse=False, nbits=nBits)\n",
    "\n",
    "def fp_morgan_sparse(mol, radius: int) -> FPResult:\n",
    "    siv = AllChem.GetMorganFingerprint(mol, radius=radius)  # UIntSparseIntVect\n",
    "    kind = \"AEs\" if radius == 1 else f\"MorganSparse_r{radius}\"\n",
    "    return FPResult(kind=kind, obj=siv, is_sparse=True)\n",
    "\n",
    "def fp_tt_sparse(mol) -> FPResult:\n",
    "    vec = rdDesc.GetTopologicalTorsionFingerprintAsIntVect(mol)\n",
    "    return FPResult(kind=\"TT\", obj=vec, is_sparse=True)\n",
    "\n",
    "def fp_ap_hashed(mol, nBits: int) -> FPResult:\n",
    "    bv = rdDesc.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=nBits)\n",
    "    return FPResult(kind=\"HashAP\", obj=bv, is_sparse=False, nbits=nBits)\n",
    "\n",
    "def fp_rdk4(mol, branchedPaths: bool = True, nBits: int = 2048) -> FPResult:\n",
    "    bv = Chem.RDKFingerprint(mol, fpSize=nBits, minPath=2, maxPath=4, branchedPaths=branchedPaths)\n",
    "    return FPResult(kind=\"RDK4\" if branchedPaths else \"RDK4-L\", obj=bv, is_sparse=False, nbits=nBits)\n",
    "\n",
    "def fp_maccs(mol) -> FPResult:\n",
    "    bv = MACCSkeys.GenMACCSKeys(mol)  # 167 bits\n",
    "    return FPResult(kind=\"MACCS\", obj=bv, is_sparse=False, nbits=167)\n",
    "\n",
    "def compute_input_fp(mol, fp_name: str, nBits: int = 2048) -> FPResult:\n",
    "    name = fp_name.upper()\n",
    "    if name == \"ECFP0\":\n",
    "        return fp_morgan_hashed(mol, radius=0, nBits=nBits, useFeatures=False)\n",
    "    if name == \"ECFP2\":\n",
    "        return fp_morgan_hashed(mol, radius=1, nBits=nBits, useFeatures=False)\n",
    "    if name == \"ECFP4\":\n",
    "        return fp_morgan_hashed(mol, radius=2, nBits=nBits, useFeatures=False)\n",
    "    if name == \"FCFP2\":\n",
    "        return fp_morgan_hashed(mol, radius=1, nBits=nBits, useFeatures=True)\n",
    "    if name == \"FCFP4\":\n",
    "        return fp_morgan_hashed(mol, radius=2, nBits=nBits, useFeatures=True)\n",
    "    if name == \"AES\" or name == \"AES\" or name == \"AEs\".upper():\n",
    "        return fp_morgan_sparse(mol, radius=1)\n",
    "    if name == \"TT\":\n",
    "        return fp_tt_sparse(mol)\n",
    "    if name == \"HASHAP\":\n",
    "        return fp_ap_hashed(mol, nBits=nBits)\n",
    "    if name == \"RDK4\":\n",
    "        return fp_rdk4(mol, branchedPaths=True, nBits=nBits)\n",
    "    if name == \"RDK4-L\":\n",
    "        return fp_rdk4(mol, branchedPaths=False, nBits=nBits)\n",
    "    if name == \"MACCS\":\n",
    "        return fp_maccs(mol)\n",
    "    raise ValueError(f\"Unknown input FP: {fp_name}\")\n",
    "\n",
    "def fp_to_tokens(fp: FPResult) -> list[int]:\n",
    "    \"\"\"Map an RDKit fingerprint to token IDs for a Transformer.\n",
    "\"\n",
    "    \"- Hashed bit vectors: return active bit indices\n",
    "\"\n",
    "    \"- Sparse vectors: return explicit keys, repeated by count (multiset)\n",
    "\"\n",
    "    \"\"\"\n",
    "    if fp.is_sparse:\n",
    "        elems = fp.obj.GetNonzeroElements()  # dict[id] = count\n",
    "        toks = []\n",
    "        for k, c in elems.items():\n",
    "            toks.extend([int(k)] * int(c))\n",
    "        toks.sort()\n",
    "        return toks\n",
    "    else:\n",
    "        return sorted(list(fp.obj.GetOnBits()))\n",
    "\n",
    "# ---- Evaluation FP (Morgan sparse r=1 for Tc) ----\n",
    "def eval_fp_morgan_sparse_r1(mol) -> FPResult:\n",
    "    return fp_morgan_sparse(mol, radius=1)\n",
    "\n",
    "def tanimoto(fp_a: FPResult, fp_b: FPResult) -> float:\n",
    "    return DataStructs.TanimotoSimilarity(fp_a.obj, fp_b.obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6047d",
   "metadata": {},
   "source": [
    "## 6) Adaptador de MolForge (conecta tu modelo aquí)\n",
    "<small>Implementa 2 métodos: `load()` (carga modelo/checkpoint/tokenizador) y `predict_batch()` (decodifica una lista de secuencias de tokens a SMILES/SELFIES).\n",
    "Este cuaderno **no incluye modo demo**: debes tener MolForge operativo para ejecutar la celda 8.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9932c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MolForgeDecoder:\n",
    "    \"\"\"Minimal adapter for the MolForge model.\n",
    "\"\n",
    "    \"You MUST implement .load() and .predict_batch() to call the real MolForge API.\n",
    "\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, checkpoint: str, device: str = \"cpu\",\n",
    "                 tokenizer_vocab_json: Optional[str] = None, output_repr: str = \"SMILES\"):\n",
    "        self.model_name = model_name\n",
    "        self.checkpoint = checkpoint\n",
    "        self.device = device\n",
    "        self.tokenizer_vocab_json = tokenizer_vocab_json\n",
    "        self.output_repr = output_repr\n",
    "        self._model = None\n",
    "        self._tokenizer = None\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load your MolForge model + tokenizer here.\n",
    "\"\n",
    "        \"Example (pseudo-code):\n",
    "\"\n",
    "        \"    from molforge.models import load_model, load_tokenizer\n",
    "\"\n",
    "        \"    self._model = load_model(self.model_name, self.checkpoint, device=self.device)\n",
    "\"\n",
    "        \"    self._tokenizer = load_tokenizer(self.model_name, vocab_json=self.tokenizer_vocab_json)\n",
    "\"\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement MolForgeDecoder.load() to load the actual model.\")\n",
    "\n",
    "    def predict_batch(self, list_of_token_lists: list[list[int]]) -> list[str]:\n",
    "        \"\"\"Decode a batch of token sequences into strings (SMILES or SELFIES).\n",
    "\"\n",
    "        \"Return one string per input sequence, in the same order.\n",
    "\"\n",
    "        \"Example (pseudo-code):\n",
    "\"\n",
    "        \"    return self._model.generate(list_of_token_lists, output_repr=self.output_repr)\n",
    "\"\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Implement MolForgeDecoder.predict_batch() to call the actual MolForge decoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698aaf0",
   "metadata": {},
   "source": [
    "## 7) Función mínima de evaluación (precisión top‑1, Tc=1.0)\n",
    "<small>Calcula la **precisión** como el % de pares (GT, pred) cuya **huella de evaluación** (`Morgan sparse r=1`) coincide exactamente (**Tc=1.0**). También devuelve cuántas predicciones fueron inválidas (no parseables).</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e29534b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_precision_tc1(\n",
    "    smiles_list: list[str],\n",
    "    input_fp: str,\n",
    "    model_name: str,\n",
    "    checkpoint: str,\n",
    "    output_repr: str = \"SMILES\",\n",
    "    device: str = \"cpu\",\n",
    "    hashed_nbits: int = 2048,\n",
    "    tokenizer_vocab_json: Optional[str] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Compute top-1 exactness (Tc=1.0) using Morgan sparse r=1 as evaluation FP.\n",
    "\"\n",
    "    \"Return a small dict with precision and simple counts.\n",
    "\"\n",
    "    \"\"\"\n",
    "    # 1) Parse GT mols and build input fingerprints (tokens) to FEED MolForge\n",
    "    gt_mols = [to_mol(s) for s in smiles_list]\n",
    "    input_tokens = []\n",
    "    for m in gt_mols:\n",
    "        if m is None:\n",
    "            input_tokens.append([])\n",
    "            continue\n",
    "        fp = compute_input_fp(m, input_fp, nBits=hashed_nbits)\n",
    "        toks = fp_to_tokens(fp)\n",
    "        input_tokens.append(toks)\n",
    "\n",
    "    # 2) Decode with MolForge\n",
    "    decoder = MolForgeDecoder(\n",
    "        model_name=model_name,\n",
    "        checkpoint=checkpoint,\n",
    "        device=device,\n",
    "        tokenizer_vocab_json=tokenizer_vocab_json,\n",
    "        output_repr=output_repr,\n",
    "    )\n",
    "    decoder.load()  # <-- implement inside the class\n",
    "    preds = decoder.predict_batch(input_tokens)  # <-- implement inside the class\n",
    "\n",
    "    # 3) Evaluate Tc=1.0 (Morgan sparse r=1) and invalids\n",
    "    exact = 0\n",
    "    invalid = 0\n",
    "    total = len(smiles_list)\n",
    "\n",
    "    for gt, pr in zip(smiles_list, preds):\n",
    "        # Convert SELFIES -> SMILES if needed for evaluation\n",
    "        if output_repr.upper() == \"SELFIES\":\n",
    "            pr = selfies_to_smiles(pr)\n",
    "\n",
    "        gt_m = to_mol(gt)\n",
    "        pr_m = to_mol(pr)\n",
    "        if gt_m is None or pr_m is None:\n",
    "            invalid += 1\n",
    "            continue\n",
    "\n",
    "        fp_gt = eval_fp_morgan_sparse_r1(gt_m)\n",
    "        fp_pr = eval_fp_morgan_sparse_r1(pr_m)\n",
    "        tc = tanimoto(fp_gt, fp_pr)\n",
    "        if tc >= 1.0 - 1e-12:\n",
    "            exact += 1\n",
    "\n",
    "    precision = exact / total if total else float(\"nan\")\n",
    "    return {\n",
    "        \"n\": total,\n",
    "        \"precision_tc1\": precision,  # e.g., ~0.93 means 93%\n",
    "        \"num_exact\": exact,\n",
    "        \"num_invalid\": invalid,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd4f49",
   "metadata": {},
   "source": [
    "## 8) Ejecutar evaluación con los SMILES de prueba\n",
    "<small>**IMPORTANTE**: Esta celda requiere que hayas implementado `MolForgeDecoder.load()` y `predict_batch()` y que `MODEL_NAME`/`MOLFORGE_CHECKPOINT` apunten a tu modelo real.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bad58be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:17:24] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:17:24] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:17:24] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:17:24] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:17:24] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Implement MolForgeDecoder.load() to load the actual model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_precision_tc1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmiles_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSMILES_LIST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_fp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_FP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMOLFORGE_CHECKPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_repr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_REPR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhashed_nbits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHASHED_NBITS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_vocab_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn               :\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_tc1   :\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_tc1\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# fraction; multiply by 100 for %\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 35\u001b[0m, in \u001b[0;36mevaluate_precision_tc1\u001b[1;34m(smiles_list, input_fp, model_name, checkpoint, output_repr, device, hashed_nbits, tokenizer_vocab_json)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 2) Decode with MolForge\u001b[39;00m\n\u001b[0;32m     28\u001b[0m decoder \u001b[38;5;241m=\u001b[39m MolForgeDecoder(\n\u001b[0;32m     29\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     30\u001b[0m     checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     output_repr\u001b[38;5;241m=\u001b[39moutput_repr,\n\u001b[0;32m     34\u001b[0m )\n\u001b[1;32m---> 35\u001b[0m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# <-- implement inside the class\u001b[39;00m\n\u001b[0;32m     36\u001b[0m preds \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mpredict_batch(input_tokens)  \u001b[38;5;66;03m# <-- implement inside the class\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 3) Evaluate Tc=1.0 (Morgan sparse r=1) and invalids\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 29\u001b[0m, in \u001b[0;36mMolForgeDecoder.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Load your MolForge model + tokenizer here.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m        \"Example (pseudo-code):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplement MolForgeDecoder.load() to load the actual model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Implement MolForgeDecoder.load() to load the actual model."
     ]
    }
   ],
   "source": [
    "\n",
    "results = evaluate_precision_tc1(\n",
    "    smiles_list=SMILES_LIST,\n",
    "    input_fp=INPUT_FP,\n",
    "    model_name=MODEL_NAME,\n",
    "    checkpoint=MOLFORGE_CHECKPOINT,\n",
    "    output_repr=OUTPUT_REPR,\n",
    "    device=device,\n",
    "    hashed_nbits=HASHED_NBITS,\n",
    "    tokenizer_vocab_json=None,\n",
    ")\n",
    "print(\"n               :\", results[\"n\"])\n",
    "print(\"precision_tc1   :\", results[\"precision_tc1\"])  # fraction; multiply by 100 for %\n",
    "print(\"num_exact       :\", results[\"num_exact\"])\n",
    "print(\"num_invalid     :\", results[\"num_invalid\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9dc642",
   "metadata": {},
   "source": [
    "## 9) Cómo conectar MolForge al adaptador (resumen rápido)\n",
    "<small>\n",
    "1. Instala el repo de MolForge y asegúrate de poder importar sus módulos (e.g., `import molforge`).\n",
    "2. En `MolForgeDecoder.load()`, carga el **modelo** y el **tokenizador** con tu checkpoint (`MOLFORGE_CHECKPOINT`) y `MODEL_NAME`.\n",
    "3. En `MolForgeDecoder.predict_batch()`, convierte `list_of_token_lists` al tipo que espera el modelo y llama al método de **decodificación/generación** (p. ej., `model.generate(...)`). Devuelve una lista de strings (SMILES o SELFIES) en el mismo orden.\n",
    "4. Verifica que tu **tokenización de entrada** coincide con la usada en el entrenamiento (para hashed: índices de bits activos; para sparse: IDs explícitos de fragmentos con multiplicidad).\n",
    "5. Lanza la celda 8 para obtener la **precisión** (Tc=1.0) del modelo pre-entrenado sobre tus SMILES.\n",
    "</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}